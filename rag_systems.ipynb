{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc7eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7987e5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Load api key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"API key loaded Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3441815",
   "metadata": {},
   "source": [
    "### Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba80690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 PDF Documents.\n"
     ]
    }
   ],
   "source": [
    "# Load PDF documents from a specified folder\n",
    "documents = []\n",
    "\n",
    "for pdf_path in glob.glob(\"documents/*.pdf\"):  #\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "\n",
    "print(f\"Loaded {len(documents)} PDF Documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874e622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 5 documents into 15 chunks\n",
      "\n",
      "Chunk 1: I, Bello Oluwadamilare, am a Data Scientist and AI Engineer with a\n",
      "specialized focus on software engineering and the development of\n",
      "intelligent systems that harmonize technical hardware principles with\n",
      "advanced predictive modeling. My expertise is centered on the\n",
      "engineering sector, where I leverage data-driven decision-making and\n",
      "applied machine learning to optimize complex systems and improve\n",
      "operational outcomes. I hold a Bachelor of Engineering (B.Eng) in\n",
      "\n",
      "Chunk 2: operational outcomes. I hold a Bachelor of Engineering (B.Eng) in\n",
      "Electrical and Electronics Engineering from the Federal University of\n",
      "Agriculture, Abeokuta, and a National Diploma in the same field from the\n",
      "Federal Polytechnic, Ilaro, providing me with a robust analytical\n",
      "foundation. Over the past few years, I have spearheaded diverse technical\n",
      "projects, including a Chicago crime data analysis for identifying\n",
      "behavioral hotspots, a predictive system for loan eligibility based on\n",
      "\n",
      "Chunk 3: credit dependencies, and the classification of power quality disturbances\n",
      "to enhance electrical grid reliability. To further my technical evolution, I\n",
      "am currently participating in an intensive Generative AI fellowship,\n",
      "focusing on the deployment of large-scale AI models and backend\n",
      "integration. Beyond my technical contributions, I demonstrated strong\n",
      "leadership as the head of my final year undergraduate project group,\n",
      "successfully coordinating team workflows and technical research. I am a\n",
      "\n",
      "Chunk 4: proponent of deep-dive root-cause analysis, ensuring that every technical\n",
      "solution I develop is preceded by a thorough diagnostic phase to ensure\n",
      "long-term efficiency and scalability. My research interests are primarily\n",
      "focused on the intersection of AI and the financial world, where I aim to\n",
      "implement automated forecasting models to solve modern fiscal\n",
      "challenges. Outside of my core professional responsibilities, I actively\n",
      "\n",
      "Chunk 5: explore the capabilities of Python, Matlab, and PowerBI, GX Developers,\n",
      "to develop innovative tools that bridge the gap between traditional\n",
      "engineering and the future of artificial intelligence.\n",
      "\n",
      "Chunk 6: Date of Birth: 8th January\n",
      "Place of Origin: Ogun State, Nigeria\n",
      "Current Location: Lagos State, Nigeria\n",
      "Languages: Fluent in English, Native in Yoruba\n",
      "Technical Proficiency: Python, Matlab, PowerBI, Software Engineering,\n",
      "Data Analytics, System Troubleshooting\n",
      "Interests: Engineering Innovation, Financial Technology (FinTech),\n",
      "Reading, System Optimization, Automation\n",
      "\n",
      "Chunk 7: Bello Oluwadamilare T.\n",
      "AI Engineer | Data Scientist | Financial Engineer\n",
      "Professional Summary:\n",
      "AI Engineer and Data Scientist with hands-on experience designing,\n",
      "training, and\n",
      "deploying machine learning and deep learning models. Proficient in\n",
      "developing scalable AI solutions, predictive analytics systems, and NLP-\n",
      "based applications.\n",
      "Skilled in Python, SQL, PyTorch, LangChain, and cloud-based ML\n",
      "pipelines, with a strong foundation in applied mathematics and\n",
      "\n",
      "Chunk 8: pipelines, with a strong foundation in applied mathematics and\n",
      "quantitative finance. Adept at transforming complex datasets into\n",
      "actionable insights and deploying models into production environments.\n",
      "Education:\n",
      " M.Sc. Automation Engineer.\n",
      " B.Eng. Electrical Electronics Engineering\n",
      " OND. Electrical Electronics Engineering\n",
      "Technical Skills:\n",
      "• Programming: Python, SQL, Solidity\n",
      "• Machine Learning: Regression, Classification, Time-Series\n",
      "Forecasting,\n",
      "Predictive Analytics\n",
      "\n",
      "Chunk 9: Forecasting,\n",
      "Predictive Analytics\n",
      "• Deep Learning: Neural Networks, Transformers, Autoencoders,\n",
      "Computer\n",
      "vision\n",
      "• Natural Language Processing (NLP): Text Embeddings, Vector\n",
      "Databases,\n",
      "Retrieval-Augmented Generation (RAG) Systems\n",
      "• Tools & Platforms: LangChain, FAISS, ChromaDB, Git, FastAPI,\n",
      "Cloud\n",
      "\n",
      "Chunk 10: ML Pipelines\n",
      "Professional Experience:\n",
      "• Developed and deployed predictive models for e-commerce sales\n",
      "forecasting, improving trend analysis and decision-making accuracy.\n",
      "• Designed AI-powered document-based chatbots using vector\n",
      "embeddings and LLMs, enhancing automated knowledge retrieval\n",
      "systems.\n",
      "• Built FASTAPIs for ML model deployment with Flask, enabling\n",
      "seamless integration of AI solutions into web and enterprise applications.\n",
      "\n",
      "Chunk 11: • Engineered end-to-end ML pipelines, including data preprocessing,\n",
      "feature engineering, model training, evaluation, and production\n",
      "deployment.\n",
      "\n",
      "Chunk 12: Research Interests & Career Goals\n",
      "My research focus involves the creation of sophisticated systems that\n",
      "integrate information retrieval with natural language processing and\n",
      "predictive modeling. I am dedicated to advancing Retrieval-Augmented\n",
      "Generation (RAG) frameworks, particularly their implementation within\n",
      "the financial, entertainment, and Web3 sectors to enhance decision-\n",
      "making and smart contract automation. Furthermore, I am committed to\n",
      "\n",
      "Chunk 13: making and smart contract automation. Furthermore, I am committed to\n",
      "utilizing AI to revolutionize electrical and automation engineering,\n",
      "specifically through the development of early fault detection systems and\n",
      "predictive maintenance models designed to minimize industrial downtime\n",
      "and maximize system reliability. My professional trajectory is aimed\n",
      "toward becoming a Lead AI Engineer, where I will architect resilient,\n",
      "high-scale platforms that enable data-centric organizational strategies. I\n",
      "\n",
      "Chunk 14: strive to contribute to the frontiers of AI research while engineering\n",
      "tangible tools that solve intricate problems in decentralized finance (DeFi)\n",
      "and intelligent industrial systems. Central to my work is a dedication to\n",
      "ethical AI and model transparency, ensuring that complex technologies\n",
      "remain accessible, responsible, and interpretable for non-technical users. I\n",
      "am driven by the goal of translating advanced AI capabilities into\n",
      "\n",
      "Chunk 15: am driven by the goal of translating advanced AI capabilities into\n",
      "significant real-world impact, bridging the gap between theoretical\n",
      "research and practical applications in both financial engineering and next-\n",
      "generation automation.\n"
     ]
    }
   ],
   "source": [
    "# Create splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=70,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf58ad",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"Who is Bello?\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a4cbd",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412855b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store from documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    collection_name=\"my_info_collection\",\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a6969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retriver\n",
    "query = \"Which School did Bello attend?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2006df9",
   "metadata": {},
   "source": [
    "### Conversational RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant answering questions about Bello Oluwadamilare T. using the provided documents.\n",
    "\n",
    "Use ONLY the context below to answer the question.\n",
    "If the answer is not in the context, say \"I don't know.\"\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in clear sentences.\n",
    "At the end, list the sources you used as bullet points.\n",
    "\"\"\")\n",
    "\n",
    "# format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata.get('source', 'unknown')}\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "# RAG chain Using LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG chain\n",
    "query = \"What ML projects has Oluwadamilare worked on?\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fb944",
   "metadata": {},
   "source": [
    "### Store Conversational RAG History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd839d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store for chat histories\n",
    "chat_store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in chat_store:\n",
    "        chat_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_store[session_id]\n",
    "\n",
    "# Create conversational prompt\n",
    "conv_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant answering questions about Olasunkanmi Akeem Rasak using the provided documents. Use ONLY the context below to answer the question. If the answer is not in the context, say I don't know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"system\", \"Answer in clear sentences. At the end, list the sources you used as bullet points.\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Build base chain\n",
    "conv_chain_base = (\n",
    "    RunnableParallel(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"question\"])),\n",
    "        question=lambda x: x[\"question\"],\n",
    "        chat_history=lambda x: x.get(\"chat_history\", [])\n",
    "    )\n",
    "    | conv_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Wrap with message history\n",
    "conv_chain = RunnableWithMessageHistory(\n",
    "    conv_chain_base,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edadf1b",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798671e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First question\n",
    "response = conv_chain.invoke(\n",
    "    {\"question\": \"Which School did Oluwadamilare attend?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "print(\"Response 1:\\n\", response)\n",
    "\n",
    "# Follow-up question\n",
    "response2 = conv_chain.invoke(\n",
    "    {\"question\": \"Which of the schools did he obtain National Diploma from?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "\n",
    "print(\"\\nResponse 2:\\n\", response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
